{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Pretty measty intro about whats going on here, why we want to do this, what it is, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: statsmodels in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.18 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from statsmodels) (1.24.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from statsmodels) (1.10.1)\n",
      "Requirement already satisfied: pandas>=1.0 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from statsmodels) (2.0.2)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from statsmodels) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from pandas>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from pandas>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from pandas>=1.0->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn\n",
    "%pip install statsmodels\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import  confusion_matrix, roc_auc_score\n",
    "from sklearn.feature_selection import RFE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You get the idea now... this is the same code as in the earlier modules. We are just applying the same preprocessing once again to clean our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a dataset from a csv file, and store it as a DataFrame in the variable df.\n",
    "# A DataFrame is a 2-dimensional labeled data structure with columns of potentially different types.\n",
    "df = pd.read_csv('heart_disease.csv')\n",
    "\n",
    "# Remove any rows from the DataFrame which contain missing values.\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define a list of column names in the DataFrame that we know to contain categorical variables.\n",
    "# Categorical variables represent types of data which may be divided into groups.\n",
    "categorical_columns = ['Gender', 'education', 'currentSmoker', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'Heart_stroke']\n",
    "\n",
    "# Loop through each column name in our list of categorical columns.\n",
    "for column in categorical_columns:\n",
    "    # Convert the data type of each column to 'category'.\n",
    "    # This tells pandas that this column should be treated as categorical data.\n",
    "    df[column] = df[column].astype('category')\n",
    "    \n",
    "# Define a list of column names in the DataFrame that we know to contain text data.\n",
    "text_columns = ['Gender', 'education', 'prevalentStroke', 'Heart_stroke']\n",
    "\n",
    "# Loop through each column name in our list of text columns.\n",
    "for column in text_columns:\n",
    "    # Convert all characters in the text to lower case.\n",
    "    # This can help with consistency when analyzing or comparing text.\n",
    "    df[column] = df[column].str.lower()\n",
    "    \n",
    "# Replace the string values 'yes' and 'no' in the 'prevalentStroke' column with 1 and 0 respectively.\n",
    "# This process is known as encoding, and it allows us to represent categorical data numerically.\n",
    "df['prevalentStroke'] = df['prevalentStroke'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Replace the string values 'yes' and 'no' in the 'Heart_stroke' column with 1 and 0 respectively.\n",
    "# This is another example of encoding categorical data.\n",
    "df['Heart_stroke'] = df['Heart_stroke'].map({'yes': 1, 'no': 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here\n",
    "### Make logistic regression basic funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary metrics for evaluation\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define a function 'doRegression' to perform logistic regression\n",
    "# The function accepts predictor variables 'x' and the outcome variable 'y'\n",
    "def doRegression(x,y):\n",
    "    # Split the data into training and testing sets using a 80-20 split\n",
    "    # The 'random_state' parameter is set to 42 for reproducibility\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Instantiate a Logistic Regression model\n",
    "    # Logistic Regression is a statistical model used to model a binary outcome variable based on one or more predictor variables\n",
    "    log_model = LogisticRegression()\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    # The model 'learns' the relationship between the predictor(s) and the outcome\n",
    "    log_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Use the model to predict outcomes for the test set\n",
    "    y_pred = log_model.predict(X_test)\n",
    "\n",
    "    # Create a confusion matrix from the actual and predicted outcomes\n",
    "    # A confusion matrix is a table that is used to evaluate the performance of a classification model. We will discuss this in detail later\n",
    "    # Its not that confusing we promise!\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Calculate predicted probabilities for the positive class\n",
    "    # These probabilities provide insight into the confidence of the predictions\n",
    "    # Need this for ROC-AUC (another metric we will talk about in detail later)\n",
    "    y_pred_proba = log_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # Compute the Area Under the Receiver Operating Characteristic Curve (ROC-AUC) from prediction scores\n",
    "    # ROC-AUC is a popular metric for evaluating classifier performance\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Return a classification report (precision, recall, f1-score, support), confusion matrix and ROC-AUC score\n",
    "    return classification_report(y_test, y_pred), conf_matrix, roc_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do init regression using 1 variable for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91       610\n",
      "           1       0.00      0.00      0.00       122\n",
      "\n",
      "    accuracy                           0.83       732\n",
      "   macro avg       0.42      0.50      0.45       732\n",
      "weighted avg       0.69      0.83      0.76       732\n",
      "\n",
      "[[610   0]\n",
      " [122   0]]\n",
      "0.6544746036011825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define the predictor (what we believe could help us predict whether a heart stroke might happen) and target variables for the analysis\n",
    "x = df[['age']]  # predictor: 'age' column from the dataframe df\n",
    "\n",
    "y = df['Heart_stroke']  # target variable (what we are interested in predicting): 'Heart_stroke' column from the dataframe df\n",
    "\n",
    "# Call the previously defined 'doRegression' function, passing the predictor and target variables as parameters\n",
    "# This function will train a logistic regression model using the given predictor and target variables, \n",
    "# and it will return a classification report (precision, recall, f1-score, support), a confusion matrix, and the ROC-AUC score.\n",
    "# See the function for details \n",
    "classification_report_output, conf_matrix, roc_auc = doRegression(x,y)\n",
    "\n",
    "\n",
    "# Print the classification report, confusion matrix, and ROC-AUC score\n",
    "\n",
    "# The classification report includes metrics such as:\n",
    "# precision (true positives divided by all positives), \n",
    "# recall (true positives divided by the number of true positives plus the number of false negatives)\n",
    "# F1 score (a weighted average of precision and recall).\n",
    "print(classification_report_output)\n",
    "\n",
    "# The confusion matrix provides a visual look at the performance of the model by showing the true positives, \n",
    "# true negatives, false positives, and false negatives.\n",
    "print(conf_matrix)\n",
    "\n",
    "# The ROC-AUC score represents the model's ability to distinguish between the positive and negative classes. \n",
    "# It ranges from 0.5 to 1, where 0.5 indicates a model that performs no better than random chance, and 1 indicates a perfect model.\n",
    "\n",
    "print(roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some markdown code explaining this data in disgusting detail including why we want to use wach metric. add pics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try another variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91       610\n",
      "           1       0.00      0.00      0.00       122\n",
      "\n",
      "    accuracy                           0.83       732\n",
      "   macro avg       0.42      0.50      0.45       732\n",
      "weighted avg       0.69      0.83      0.76       732\n",
      "\n",
      "[[610   0]\n",
      " [122   0]]\n",
      "0.4885245901639344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define predictor and target variables\n",
    "x = df[['currentSmoker']]  # Define the predictor (what we believe could help us predict whether a heart stroke might happen)\n",
    "\n",
    "y = df['Heart_stroke']  # target variable (what we are interested in predicting): 'Heart_stroke' column from the dataframe df\n",
    "\n",
    "classification_report_output, conf_matrix, roc_auc = doRegression(x,y) # Do regression \n",
    "\n",
    "\n",
    "# Print outputs\n",
    "print(classification_report_output)\n",
    "print(conf_matrix)\n",
    "print(roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try doing it with 2 variables now to see if that increaseS AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91       610\n",
      "           1       0.00      0.00      0.00       122\n",
      "\n",
      "    accuracy                           0.83       732\n",
      "   macro avg       0.42      0.50      0.45       732\n",
      "weighted avg       0.69      0.83      0.76       732\n",
      "\n",
      "[[610   0]\n",
      " [122   0]]\n",
      "0.6289035205589895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define predictor and target variables\n",
    "x = df[['BPMeds','prevalentHyp']]  # predictor\n",
    "y = df['Heart_stroke']  # target variable (what we are interested in predicting): 'Heart_stroke' column from the dataframe df\n",
    "\n",
    "classification_report_output, conf_matrix, roc_auc = doRegression(x,y) # Do regression \n",
    "\n",
    "# Print outputs\n",
    "\n",
    "print(classification_report_output)\n",
    "print(conf_matrix)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Didnt really go up that much. Bummer. Should we try and force every combination of column data we can to increase our predictor? We could, OR we could use a machine learning tool called recusive feature elimination (RFE).\n",
    "\n",
    "### We will initially train the model with every feature (variable), and then gradually drop out the least important ones until we have distilled down the most important ones (we can set the number of how many features we want left). In Python, a lot of this happens under the hood so you will mostly just see the syntax, but this is a process called *feature engineering*. There are tons of ways to do this, and the *right* way depends on the specific data you are using coupled with your own domain knowledge. RFE is not right in every case, but we use it here because it is easy to understand and a great introduction to feature engineering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make function for log regression with recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# This function performs Logistic Regression with Recursive Feature Elimination (RFE) to select the top 5 most relevant features\n",
    "def doRegressionWithRFE(x,y):\n",
    "    \n",
    "    # Split the input data into training and testing datasets, with 80% of data used for training and 20% used for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the Logistic Regression model\n",
    "    log_model = LogisticRegression()\n",
    "\n",
    "    # Initialize RFE with the logistic regression model and specify to select top 5 features\n",
    "    selector = RFE(log_model, n_features_to_select=5, step=1)\n",
    "\n",
    "    # Fit the RFE selector on the training data\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "\n",
    "    # Print the features selected by RFE. This is achieved by creating a series combining feature names and their selection status, then printing those selected.\n",
    "    selected_features = pd.Series(selector.support_, index=X_train.columns)\n",
    "    print(selected_features[selected_features==True])\n",
    "\n",
    "    # Transform the training and test data sets so only the selected features remain\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "\n",
    "    # Fit the Logistic Regression model on the selected features of the training data\n",
    "    log_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Predict the outcomes for the test set\n",
    "    y_pred = log_model.predict(X_test_selected)\n",
    "\n",
    "    # Print the confusion matrix, which gives a summary of the prediction results on the test data\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Compute the predicted probabilities for the positive class\n",
    "    y_pred_proba = log_model.predict_proba(X_test_selected)[:,1]\n",
    "\n",
    "    # Print the model's accuracy on the test set\n",
    "    print('score', log_model.score(X_test_selected,y_test))\n",
    "\n",
    "    # Compute ROC-AUC score, which quantifies the model's ability to distinguish between classes\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Return a detailed classification report, the confusion matrix, and the ROC-AUC score\n",
    "    return classification_report(y_test, y_pred), conf_matrix, roc_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do log regression using feature selection and one hot encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prevalentStroke    True\n",
      "Gender_female      True\n",
      "BPMeds_0.0         True\n",
      "prevalentHyp_1     True\n",
      "diabetes_0         True\n",
      "dtype: bool\n",
      "score 0.8319672131147541\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91       610\n",
      "           1       0.40      0.02      0.03       122\n",
      "\n",
      "    accuracy                           0.83       732\n",
      "   macro avg       0.62      0.51      0.47       732\n",
      "weighted avg       0.76      0.83      0.76       732\n",
      "\n",
      "[[607   3]\n",
      " [120   2]]\n",
      "0.6435232464391293\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical variables in the dataset into indicator (or dummy) variables. Each unique value in the original column is\n",
    "# expanded into a new column that takes binary values (0, 1)\n",
    "#  gender   -->one hot encoding --> gender_male  gender_female\n",
    "#0   male                               1              0\n",
    "#1 female                               0              1\n",
    "#2   male                               1              0\n",
    "#3 female                               0              1\n",
    "df_encoded = pd.get_dummies(df)\n",
    "\n",
    "# Call the function to perform Logistic Regression with RFE\n",
    "# For predictors (x), we use all columns in the dataframe after converting categorical variables to dummy variables, except the target column \n",
    "# 'Heart_stroke'. We didn't do this before because some categorical variable's weren't appropriate for the regression. We have fixed that now by using one hot encoding\n",
    "x = df_encoded.drop(columns=['Heart_stroke'])\n",
    "\n",
    "# The target variable (y) is 'Heart_stroke'\n",
    "y = df_encoded['Heart_stroke']\n",
    "\n",
    "# Call the function defined previously, passing in the predictor variables and the target variable\n",
    "# The function returns a classification report, a confusion matrix, and the ROC-AUC score\n",
    "classification_report_output, conf_matrix, roc_auc = doRegressionWithRFE(x,y)\n",
    "\n",
    "# Print the results\n",
    "print(classification_report_output)\n",
    "print(conf_matrix)\n",
    "print(roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the number of true positive results divided by the number of all positive results, including those not identified correctly, and the high precision relates to the low false positive rate. For instance, the precision for the '0' class is 0.83, meaning that 83% of the instances that were predicted as '0' were actually '0'.\n",
    "\n",
    "Recall (Sensitivity) - the number of true positive results divided by the number of all samples that should have been identified as positive. For instance, the recall for the '0' class is 1.00, meaning that all the instances of '0' were identified correctly.\n",
    "\n",
    "The F1-score is the harmonic mean of precision and sensitivity. It tries to find the balance between precision and recall. It is most useful in uneven class distribution problems as it seeks a balance between Precision and Recall in an unbalanced dataset.\n",
    "\n",
    "The support is the number of occurrences of the given class in your dataset (e.g., there are 610 instances of '0' and 122 instances of '1').\n",
    "\n",
    "The accuracy of the model is 0.83, meaning that it correctly predicted the outcome 83% of the time for your test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recursive Feature Elimination (RFE) algorithm works by fitting the model, evaluating its performance, and then recursively removing the least important features until the desired number of features is left. The criterion for \"importance\" depends on the specific model used. For instance, in the case of logistic regression, it's the absolute values of the coefficients.\n",
    "\n",
    "However, feature selection doesn't always lead to better performance. It is fundamentally a search problem in the space of feature subsets, and there's no guarantee that the search strategy of RFE (or any other feature selection algorithm) will find the optimal subset for your specific model and dataset.\n",
    "\n",
    "Here are a few reasons why your precision did not increase after using RFE:\n",
    "\n",
    "Model Bias: The RFE algorithm relies on the model for estimating feature importance. If the model has any bias towards certain types of features, RFE might not be able to select the best feature set.\n",
    "\n",
    "Interactions Among Features: RFE, in its simplest form, assumes that features contribute to the model independently of each other. If there are important interactions among features, RFE might miss them.\n",
    "\n",
    "Overfitting or Underfitting: Selecting fewer features can lead to underfitting if the model becomes too simple to capture the underlying structure of the data. On the other hand, selecting too many features can lead to overfitting, especially if some of the features are noisy or irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
