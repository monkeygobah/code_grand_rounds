{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Heart Attack: K- Nearest Neighbors**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Intro** \n",
    "\n",
    "### K-Nearest Neighbors is a supervised learning classification algorithm. It's a supervised algorithm, because we start with data that is already labeled and learn from that data. We start the algorithm by choosing a number, $k$. Then, we train the model by \"plotting\" the training data and grouping them by their true label. That's it. Very simple and easy. \n",
    "\n",
    "### In order to classify a new point, we plot the new unknown data and then check the label of its $k$ nearest neighbors or training data points we previously plotted. These labels now \"vote\" on what the unknown data point should be.\n",
    "\n",
    "![knn.png](attachment:knn.png)\n",
    "\n",
    "### Something to keep in mind: the size of $k$ will matter for accuracy and bias. The smaller $k$ is the more likely an outlier may affect the classification of a new data point. The larger the $k$ is, the more likely it will encompass multiple groups and it becomes less powerful as a classification tool as you can imagine. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data and preprocess. Make sure you run this cell, but this is the same code from the data exploration notebook. We are just getting the data into the same form so we can continue to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Splitting the Data**\n",
    "\n",
    "### Mention how this came up in module 2, but go into a little more detail here as it is a key aspect of machine learning going forward\n",
    "\n",
    "### For machine learning, we need to split our data into train and test subsets. We do this in order to first teach our model trends to look for using the train dataset. Then, we use the test data to verify our model learned the correct trends. \n",
    "\n",
    "### Discussion on why we do 80/20 split\n",
    "\n",
    "### The first thing we have to do is take the labels we count as \"truth\" which in our case is our target column. All of the other columns will be our \"features\". These features are what the machine learning model takes and tries to learn from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target variable (heart attack)\n",
    "y = df[\"target\"]\n",
    "\n",
    "# define predictor variables or features which we will train with \n",
    "basic = df[[\"age\", 'chol', 'thalach']]\n",
    "\n",
    "# split data into train and test sets using built in method from sci-kit learn\n",
    "X_train, X_test, y_train, y_test = train_test_split(basic, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check to see if the split was done evenly. If by chance one case is significantly more represented, we can re-split the data by running it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "1    131\n",
      "0    111\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating a KNN Model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we doing here \n",
    "\n",
    "### Let's build the model using 10 neighbors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[24  3]\n",
      " [ 4 30]] \n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87        27\n",
      "           1       0.91      0.88      0.90        34\n",
      "\n",
      "    accuracy                           0.89        61\n",
      "   macro avg       0.88      0.89      0.88        61\n",
      "weighted avg       0.89      0.89      0.89        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_predicted = knn.predict(X_test)\n",
    "knn_conf_matrix = confusion_matrix(y_test, knn_predicted)\n",
    "\n",
    "\n",
    "### Introduce AUROC HERE\n",
    "\n",
    "print(\"Confusion Matrix: \\n\", knn_conf_matrix, '\\n')\n",
    "\n",
    "print(\"Classification Report: \\n\", classification_report(y_test,knn_predicted))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poor results- scaling and briefly mention it as it relates to things involving distance. This concept comes up again and again ( k means, svm) which will see later so try to understand it early!\n",
    "\n",
    "### After scaling the data, the features will now be considered the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an object of standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "###FIX THIS TO AVOID DATA LEAKAGE\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of workflow of scaling when using training and testing sets to avoid data leakage\n",
    "\n",
    "### lets now fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confussion matrix\n",
      "[[21  6]\n",
      " [ 9 25]]\n",
      "\n",
      "\n",
      "Accuracy of K-NeighborsClassifier: 75.40983606557377 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74        27\n",
      "           1       0.81      0.74      0.77        34\n",
      "\n",
      "    accuracy                           0.75        61\n",
      "   macro avg       0.75      0.76      0.75        61\n",
      "weighted avg       0.76      0.75      0.75        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a knn classifier using 10 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Train the model using our scaled training and testing data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the trained model\n",
    "knn_predicted = knn.predict(X_test)\n",
    "\n",
    "# calculate \n",
    "knn_conf_matrix = confusion_matrix(y_test, knn_predicted)\n",
    "\n",
    "\n",
    "\n",
    "print(\"confussion matrix\")\n",
    "print(knn_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,knn_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create another model, but this time using the entire set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"target\"]\n",
    "feats = df.drop('target',axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feats, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confussion matrix\n",
      "[[24  3]\n",
      " [ 4 30]]\n",
      "\n",
      "\n",
      "Accuracy of K-NeighborsClassifier: 88.52459016393442 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87        27\n",
      "           1       0.91      0.88      0.90        34\n",
      "\n",
      "    accuracy                           0.89        61\n",
      "   macro avg       0.88      0.89      0.88        61\n",
      "weighted avg       0.89      0.89      0.89        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a knn classifier using 10 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Train the model using our scaled training and testing data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the trained model\n",
    "knn_predicted = knn.predict(X_test)\n",
    "\n",
    "# calculate \n",
    "knn_conf_matrix = confusion_matrix(y_test, knn_predicted)\n",
    "\n",
    "\n",
    "\n",
    "print(\"confussion matrix\")\n",
    "print(knn_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,knn_predicted))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of these results in detail "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
