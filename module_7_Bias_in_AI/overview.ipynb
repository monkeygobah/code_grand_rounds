{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bias in AI and Algorithms**\n",
    "\n",
    "### As the role of AI and machine learning continues to grow in our day to day life from advertisements to heathcare, so does the importance of ensuring the decisions of these systems fair and unbiased. However, despite best intentions this is not always the case. It is important to recognize, that at a high level machines don't have beliefs, feelings, or prejudices. Instead, bias in AI typically arises from the data used to train these models or from the people who create them. The introduction of bias in AI can be attributed to various things ranging from a lack of proper training data causing the AI to learn based on non representive data to human subjectivity when evaluating models and changing hyperparameters. \n",
    "\n",
    "### Whether intentional or not, it is important to talk about and understand the consequences of these bias when building and evaluating AI systems. Whether it is advertisements or healthcare, AI should serve as a tool to push forward the whole human race and not continually perpetuate inequities. In fact, when built properly and trained in an ethical and conscientious manner AI systems have the potential to actually reduce disparities in the healthcare system. Dr. Michael Abramoff of The Univesity of Iowa has widely published and spoken on this if you are looking for examples.\n",
    "\n",
    "### However, in the next few lessons we are going to do a deep case study on bias in AI by reviewing a seminal paper (Obermeyer et al 2019) that prompted both academic and governmental committees to form around addressing bias in AI. In this paper they do a deep dive on an algorithm that affected more than 200,000,000 people, but was fundamentally biased in a way that was not immediately obvious from the outset. This isn't just about one algorithm but reflects a broader challenge in the AI industry. These algorithms' methodologies and assumptions are ubiquitous, found across sectors. The potential biases identified in healthcare, a critical sector affecting lives directly, certainly operate in other industries too.\n",
    "\n",
    "### We hope this serves as a reminder to all readers that no matter what your interaction with AI, whether it be development, deployment, or evaluation, that this is the number one concern. This technology, while exciting, should not developed for excitment alone, but rather to work to minimize disparities in the healthcare system and result in higher quality care at lower costs for everyone. The specific paper we are reviewing is titled: Dissecting racial bias in an algorithm used to manage the health of populations and implications and you can access it here:  https://www.science.org/doi/full/10.1126/science.aax2342. We have pasted the abstract below!\n",
    "\n",
    "Abstract\n",
    "Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of : Dissecting racial bias in an algorithm used to manage the health of populations and implications: https://www.science.org/doi/full/10.1126/science.aax2342\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "### Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.\n",
    "\n",
    "### TLDR: The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
