{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Obermeyer et. al. 2019- Proving Alternate Label Choices Work**\n",
    "\n",
    "### To further study the effect of label choice on predictive performance and racial bias the researchers constructed three predictive algorithms. Each was designed to predict distinct outcomes that might be reasonable labels with which to enroll high risk patients in the program:\n",
    "\n",
    "### 1. The total cost incurred in the following year,\n",
    "### 2. The avoidable cost in the subsequent year, primarily resulting from emergency visits and hospitalizations,\n",
    "### 3. The health of an individual in the coming year determined by the number of chronic conditions anticipated to manifest.\n",
    "\n",
    "### If you just finished Code Grand Rounds, what this means is that they split the same dataset into training and test, and based on the training data tried to predict these three different things (in separate algorithms). This means they used the health information from earlier years to predict health outcomes in later years. Race was still excluded as an input. \n",
    "\n",
    "### The algorithms delivered reasonably accurate predictions for the outcomes they were designed for as well as the other outcomes. A deeper dive, however, revealed significant variations in the racial composition of individuals identified as high-risk when using the different labels. For instance when using total cost as the label, the fraction of Black patients enrolled in the program was 14.1%, but when predicting based on chronic conditions it was  26.7% . Such marked differences emphasize that while multiple label choices might seem justifiable when intially creating an algorithm, they can yield wildly different results (nearly twofold) which has the real world outcome of patients that need care not receiving it. \n",
    "\n",
    "### Also, keep in mind that all three of the algorithms were good algorithms. They performed well when evaluated on the test set following training, and all of the labels used in the experiments are entirlely valid labels. What this demonstrates is that we have the ability to build algorithms that will make predictions with a high quality, but what we choose to make the predictions can sometimes be the relevant part. If we fail to take into consideration the fact that their exists systemic disparities which will perpetuate through the algorithm.\n",
    "\n",
    "### While this may seem disheartening and scary (particularly as, on the outset, the choice of label to be cost seemed reasonable on paper), these data are actually highly encouraging, as they show that the problem is solvable, and easily so in this case. They did not change anything about the algorithm itself (architecturem hyperparameters, etc) except the labels of the data they gave as input, and that was all it took to make the algorithm equitable. These problems if not addressed, could lead to massive risk in all aspects of life. However, as a medical community we can leverage our domain knowledge, realtinoships with colleagues and their lived experiences, and our relationships with the communities we serve to work towards building equitable algorithms for an equitable future. This is a team effort, but it is an imperative one if we hope to enjoy the benefits of AI as a society. No one can win if anyone is losing.\n",
    "\n",
    "### We hope you enjoyed this recap! We highly encourage you to check out the original paper here: https://www.science.org/doi/10.1126/science.aax2342 and give it a read for yourself.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
