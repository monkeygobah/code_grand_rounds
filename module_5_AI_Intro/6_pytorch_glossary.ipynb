{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a glossary of PyTorch tools, some that we cover and some that we don't.\n",
    "### We wanted to provide you with a repository of useful things that PyTorch gives you in case it is helpful\n",
    "### Remember, you can find all of this in the documentary or on Google as well! And as always, make sure you read a little bit before going plugging and chugging blindly. This is not an exhaustive list, so make sure you do your homework before breaing something!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Components**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies a linear transformation to the input data\n",
    "linear_layer = torch.nn.Linear(input_size, output_size)\n",
    "\n",
    "# Convolutional layer\n",
    "conv_layer = torch.nn.Conv2d(in_channels=3, out_channels=9, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Max pooling layer\n",
    "max_pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# Average pooling layer\n",
    "avg_pool = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# Dropout layer (regularization technique to prevent overfitting)\n",
    "dropout_layer = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "# Dropout layer for 2D inputs (like images)\n",
    "dropout2d_layer = torch.nn.Dropout2d(p=0.5)\n",
    "\n",
    "# Batch normalization layer (improves speed, performance, and stability of neural networks)\n",
    "batch_norm = torch.nn.BatchNorm1d(num_features=10)\n",
    "\n",
    "# Layer normalization layer\n",
    "layer_norm = torch.nn.LayerNorm(normalized_shape=10)\n",
    "\n",
    "# LSTM layer (Long Short Term Memory - type of recurrent layer)\n",
    "lstm_layer = torch.nn.LSTM(input_size=10, hidden_size=20, num_layers=2)\n",
    "\n",
    "# GRU layer (Gated Recurrent Unit - type of recurrent layer)\n",
    "gru_layer = torch.nn.GRU(input_size=10, hidden_size=20, num_layers=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loss Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error Loss: Used in regression tasks\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Cross Entropy Loss: Used in classification tasks\n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Binary Cross Entropy Loss: Used in binary classification tasks where model outputs a probability \n",
    "binary_cross_entropy_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Negative Log Likelihood Loss: Often used in conjunction with a Softmax Layer in classification tasks\n",
    "nll_loss = torch.nn.NLLLoss()\n",
    "\n",
    "# Hinge Embedding Loss: Useful for measuring whether two inputs are similar or dissimilar\n",
    "hinge_embedding_loss = torch.nn.HingeEmbeddingLoss()\n",
    "\n",
    "# Smooth L1 Loss: A combination of L1 and L2 loss, less sensitive to outliers than the MSELoss\n",
    "smooth_l1_loss = torch.nn.SmoothL1Loss()\n",
    "\n",
    "# Soft Margin Loss: A variant of SVM loss for binary classification\n",
    "soft_margin_loss = torch.nn.SoftMarginLoss()\n",
    "\n",
    "# Multi Label Margin Loss: Creates a criterion that optimizes a multi-class multi-classification hinge loss \n",
    "multi_label_margin_loss = torch.nn.MultiLabelMarginLoss()\n",
    "\n",
    "# Cosine Embedding Loss: Measures the cosine distance between two vectors\n",
    "cosine_embedding_loss = torch.nn.CosineEmbeddingLoss()\n",
    "\n",
    "# Multi Margin Loss: Creates a criterion that optimizes a multi-class classification hinge loss\n",
    "multi_margin_loss = torch.nn.MultiMarginLoss()\n",
    "\n",
    "# Triplet Margin Loss: Measures the relative similarity between samples. Useful for learning embeddings\n",
    "triplet_margin_loss = torch.nn.TripletMarginLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optimizers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent: Simple optimizer that can escape shallow local minima\n",
    "sgd_optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Adam Optimizer: Efficient and popular choice for deep learning\n",
    "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define a model parameter\n",
    "params = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# SGD with Momentum: Variation of SGD, momentum helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9)\n",
    "\n",
    "# Adagrad: Adapts the learning rate to the parameters, performs larger updates for infrequent parameters and smaller updates for frequent parameters. Good choice for sparse data.\n",
    "optimizer = torch.optim.Adagrad(params, lr=0.01)\n",
    "\n",
    "# RMSprop: Divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight. Good choice for recurrent neural networks.\n",
    "optimizer = torch.optim.RMSprop(params, lr=0.01)\n",
    "\n",
    "# Adam: Computes adaptive learning rates for different parameters. Combination of RMSprop and Momentum.\n",
    "optimizer = torch.optim.Adam(params, lr=0.01)\n",
    "\n",
    "# Adamax: It is a variant of Adam based on the infinity norm.\n",
    "optimizer = torch.optim.Adamax(params, lr=0.01)\n",
    "\n",
    "# Adadelta: It is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients.\n",
    "optimizer = torch.optim.Adadelta(params, lr=0.01)\n",
    "\n",
    "# ASGD: Averaged Stochastic Gradient Descent, it averages the parameter vector over time, leading to better generalization.\n",
    "optimizer = torch.optim.ASGD(params, lr=0.01)\n",
    "\n",
    "# LBFGS: Limited-memory BFGS, it approximates the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm using a limited amount of computer memory. Good for small datasets.\n",
    "optimizer = torch.optim.LBFGS(params, lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Activation Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Activation Function: Transforms tensor values between 0 and 1\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "# ReLU Activation Function: Replaces all negative values in the tensor with zeros\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# Tanh activation function\n",
    "tanh_activation = torch.nn.Tanh()\n",
    "\n",
    "# Softmax activation function\n",
    "softmax_activation = torch.nn.Softmax(dim=1)  # Dim indicates the dimension along which softmax will be computed\n",
    "\n",
    "# Softplus activation function\n",
    "softplus_activation = torch.nn.Softplus()\n",
    "\n",
    "# Leaky ReLU activation function\n",
    "leaky_relu_activation = torch.nn.LeakyReLU()\n",
    "\n",
    "# ELU activation function\n",
    "elu_activation = torch.nn.ELU()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
