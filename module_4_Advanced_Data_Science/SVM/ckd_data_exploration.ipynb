{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Exploration: Chronic Kidney Disease**\n",
    "\n",
    "### Welcome to the section on Support Vector Machines (SVMs)! SVMs are one of the most vertaile machine learning models and have wide utility across both regression and classification tasks. They are a true staple in any data scientists tool kit and, to this day, are still used in very advanced applications.\n",
    "\n",
    "### We will be making an SVM today for a classification task. Make sure you still remember the high level difference between classification and regression as this is guiding light when thinking about how to solve problems.\n",
    "\n",
    "### - `Classification`- whether or not something belongs to a certain class (car, truck, plane)\n",
    "\n",
    "### - `Regression`- prediction of a continuous variable (heart rate given certain parameters, post operative systolic blood pressure)\n",
    "\n",
    "### Here, we will be doing `binary classification`, and attept to predict whether or not individuals will develop kidney disease from based on various clinical observations. The data is open source and can be found here: https://www.kaggle.com/datasets/mahmoudlimam/preprocessed-chronic-kidney-disease-dataset. \n",
    "\n",
    "### We won't go to in depth with describing the data here, but one thing of note is that this dataset is already heavily preprocessed so we will have a lot of work saved for us. Missing values were calculated using KNN (see the KNN notebook in this section for more information on this!) and the categorical variables are already one hot encoded. You can read more about the other preprocessing done here: https://www.kaggle.com/code/mahmoudlimam/chronic-kidney-disease-clustering-and-prediction/notebook.\n",
    "\n",
    "### As the point of this specific section is to delve into SVMs, we felt OK using this dataset, but keep in mind that real world data is *never* this clean coming in and will always require you to do some form of preprocessing on it. Throughout all of Code Grand Rounds we show multiple different approaches to this preprocessing so it will be lighter in this section, but remember never to skip this step! Alright, without further ado lets dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import all the neccessary stuff\n",
    "\n",
    "%pip install seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data and take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "df = pd.read_csv('CKD_Preprocessed.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some basic summary statistics about the data. We will leave you to interpret these on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic summary statistics for numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some info about data types and probe number of missing (null) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get info about data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Initial Visualizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make histogram of all the data to see the distribution. Our data is all numerical (as seen by df.info) so a single histogram should suffice. For the binary data (1s or 0s) we want to see if there is a relatively even distribution of both 1s and 0s, particularly for our target (CKD status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the columns with datatypes 'float64' and 'int64' from the DataFrame 'df'.\n",
    "# For these selected columns, create histograms.\n",
    "# figsize=(20,15) sets the width and height of the entire figure in inches.\n",
    "# bins=20 means each individual histogram will have 20 bins.\n",
    "df.select_dtypes(include=['float64', 'int64']).hist(figsize=(20,15), bins=20)\n",
    "\n",
    "# Adjust the padding between and around the subplots (histograms) for a cleaner look.\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure containing the histograms.\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will leave the interpretation of a majority of these data up to you, but we want to briefly discuss our target vairable 'Chronic Kidney Disease: Yes'. \n",
    "\n",
    "### We see from the histogram in the bottom right that 250 people with kidney disease and 150 without. In this case the majority class is people with kidney disease, but this still about a 5:3 class imbalance. As indicated in the logistic regression, imbalanced classes can be a big problem in machine learning (particular classification tasks) as they can accidentally bias the model towards the majority class. This can falsely lead you to an increase in accuracy when your model is actually just predicting the majority class a majority of the time and achieving what seems like a good result. This is why it is imperative to always check other metrics like precision, recall, AUROC, etc., and be sure to think critically about the distribution of your data. We need to keep this potential imbalance in mind going forward!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data visualization\n",
    "\n",
    "### While this might not be strictly neccesary (although always a good idea...), but lets also make a correlation matrix to see if any of our features are highly correlated. This is the same code from the data exploration notebook in module 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr() # This line computes the correlation matrix of the DataFrame.\n",
    "                 #  It calculates the Pearson correlation coefficient for each pair of numerical columns. \n",
    "                 # Post cleaning, all of our columns have some kind of numerical representation.\n",
    "\n",
    "print (corr_matrix)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) #  Here, create a mask for the upper triangle of your correlation matrix. \n",
    "                                               # This is done because the matrix is symmetric, i.e., the lower triangle is a mirror \n",
    "                                               # image of the upper triangle. Thus, showing both would be redundant.\n",
    "                                               # You don't technially need to do this, but its a nice trick...\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(18, 15))\n",
    "\n",
    "# Generate a colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask\n",
    "# Look at the sns documenttion for details on all of the arguments. \n",
    "sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While we will not do a detailed assessment of this data, right away we can see that there are many features that seem to be highly correlated with our target variable. This could mean that we may be able to achieve good predictive power with this dataset, but it is also worth noting that multicollinearity can sometimes lead to overfitting. But overall, this looks good!   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Moving on**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That is really all we are going to do here. As this is a special dataset because it was pretty heavily preprocessed for us (as you know by now, this will definitely not always be the case...), we just wanted to take an initial look to 'get to know' the data before we run off and try to build models for it. Knowing the distribution is important because certain models require certain distributions to work properly, and it is always nice to know if things correlate and kind of 'make sense' within your dataset. Lets go make a model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
