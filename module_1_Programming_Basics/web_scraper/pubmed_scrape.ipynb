{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Pubmed To Fastrack Literature Searches\n",
    "## Lets say you are a med student tasked with doing a lit review, or a scientist interested in certain topics. You could use a plethora of the free tools available (Pubmed, google scholar, elicit, etc), but if you are lazy and want to automate some of the hard work, you could also build a web scraper in Python that will expedite this process for you.\n",
    "## Here we will go through the construction of a very basic web scraper to dig through Pubmed and return articles and some metadata about them automatically. Our topic will be AI in medical education"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependecies. Biopython is a set of python tools for biologic computation and you import it using Bio. Entrez is a search engine that provides access to manu biomedical databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (1.81)\n",
      "Requirement already satisfied: numpy in /Users/georgienahass/opt/anaconda3/envs/mnist/lib/python3.11/site-packages (from biopython) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install biopython\n",
    "\n",
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this project we will be using functions. Functions are a tool in Python to neatly package code that you will reuse over and over again. We will be making a 'search' function to probe the database and a 'get_details' functino to retrieve metadata about the identified articles. There is information about the code in commented text and we also provide explanations above each block. Functions are great tools in Python because they allow you to build blocks of code that are versatile and can be used many times. Say you have a bunch of lists you need to iterate over- instead of making a ton of for loops you could make one 'iterate' function and then call the function each time you have to use it. Functions are a power tool in the computer scientists toolkit, and for the duration of Code Grand Rounds (and in your own work) you will see them everywhere. It always good to try and write 'clean' code, and functinos are a great step towards doing that.\n",
    "\n",
    "### That being said, as we are building functions now and calling them later the workflow of the code might not make complete sense until you see it all in action, so just hang in there and try to understand what the functions are doing for now.\n",
    "### We will start with building our search function which will help us send some parameters to the database and retrieve a list of relevant articles.\n",
    "\n",
    "### def search(query, min_date, max_date, number):' This line is declaring our function named \"search\" which takes four parameters explained below. Remember that you declare functions in Python using the 'def' keyword\n",
    "\n",
    "- ### 'query': The topic or keywords we're interested in.\n",
    "- ### 'min_date': The earliest publication date we want in our results.\n",
    "- ### 'max_date': The latest publication date for our search.\n",
    "- ### 'number': How many articles do we want to fetch?\n",
    "\n",
    "### We then set up the search using 'search_setup = Entrez.esearch' from the Entrez module. Itâ€™s a little like walking up to a librarian (in this case, PubMed's database) and handing them a note with what we're looking for. All of the paramters are used in within this esearch call, and we have extensively commented the code in case you get confused.\n",
    "The search function returns the list of articles (pubmed IDs) that were identified! Remember to get data back from functions we have to use the 'return \"stuff to return\"' keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query- keywords you want to search\n",
    "# min_date- earliest date to start the search\n",
    "# max_date- latest date to end the search\n",
    "# number- number of articles to return\n",
    "def search(query, min_date, max_date, number):\n",
    "        #db: the database we want to search. Find more about the various database options in table 1 of 'A General Introduction to the E-utilities' paper\n",
    "    search_setup = Entrez.esearch(db='pubmed', \n",
    "             # An email is required in case something goes wrong\n",
    "            email='gnahas2@uic.edu', \n",
    "             # This decides how you want to sort the identified articles\n",
    "            sort='relevance',\n",
    "            retmax=number,\n",
    "            mindate = min_date, \n",
    "            maxdate = max_date, \n",
    "            term=query) \n",
    "    results = Entrez.read(search_setup) \n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we identified the list of pubmed IDs that are relevant to our query we want to go and retrieve the metadata associated with these articles such as authors, titles, keywords, etc. We will build a function 'fetch_details' to handle this for us. It will take as input the lsit of pubmed IDs we calculated from the 'search' function and return all of this metadata in a dictionary style format. \n",
    "\n",
    "### Even though this is a more advance Python algorithm, you will see how basic concetps in Python keep popping up. We submit a list of PubMed IDs to the database, but it takes as input a list of comma separated strings. We can easily generate this from a list using the 'join' command on our input list! This function will return the metadata in a specific data type specific to BioPython which we will talk about soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_list- list of pubmed IDs that were identified earlier\n",
    "def fetch_details(id_list):\n",
    "    # make a new single string of all the ids separated by a comma in id_list to submit\n",
    "    ids = ','.join(id_list) \n",
    "     #db: the database we want to search. Find more about the various database options in table 1 of 'A General Introduction to the E-utilities' paper\n",
    "    fetch_setup = Entrez.efetch(db='pubmed', \n",
    "           # An email is required in case something goes wrong\n",
    "            email='gnahas2@uic.edu',  \n",
    "            # ids we will be searching for in the database\n",
    "            id=ids) \n",
    "    results = Entrez.read(fetch_setup) #this stores the results of the metadata of the pubmed IDs in a pretty messy format, but one that is accessible to the computer! Read below to see how to extract it\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Detour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next functions we will build will be utilized to extract the data from our 'fetch_detai1ls' function. Before we go to much into the details of the functions, we will first talk about how BioPython and Entrez return data to us as users from the database\n",
    "\n",
    "### First, it's important to understand the context. Biopython is a set of tools and libraries for computational biology and bioinformatics in Python. One of its many functionalities is to provide ways to interface with biological databases, such as those provided by the NCBI (National Center for Biotechnology Information).\n",
    "\n",
    "### Entrez is NCBI's database system, and Biopython provides a module, Bio.Entrez (which we imported behind the scenes for you), to interface with it. When you make a query to Entrez using Biopython's tools, the results are often returned in a structured format that you can then manipulate with Python.\n",
    "\n",
    "### The DictionaryElement class in Bio.Entrez.Parser represents a specific way that Biopython structures the data it gets back from Entrez. In simpler terms, when you fetch data from Entrez using Biopython, you often get back complex nested structures of data. These structures can include lists, dictionaries, and other custom data types that Biopython provides to make it easier to navigate the data. One of these custom data types is DictionaryElement.\n",
    "\n",
    "### When you encounter a DictionaryElement, you can typically interact with it as you would with a standard Python dictionary, using keys to access values, iterating over its items, etc. However, if there are additional methods or attributes provided by the DictionaryElement class, you would need to refer to Biopython's documentation or utilize Python's introspection tools (like the dir() function) to understand and leverage them.\n",
    "\n",
    "### WHen it comes down to it, you will not encounter dictinoary elements that often, but it is how the data from BioPython get's returned, and knowing that more complicated data structures such as this exist is important. If it doesn't make complete sense, that is *totally* OK! When you encounter weird data structures like this and get confused, the first step is to not panic, and the second is to try and get an idea conceptually of how the data is stored to figure out how to get what you want. Tools like ChatGPT and Stack Overflow are great resources for things like this!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok now lets build some of the extraction functions. As mentioned earlier, the point of this module is about continuing to introduce you to python concepts, but also getting you used to looking at more complex code and trying to reason through (with our help) how and why it is working. Remember to add print statements if you are interested and check out what is going on under the hood. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start to build the our function to extract the authors. This function will take as input `medline`. You will see where we get this from in a bit, but in the Dictinoary Element from Biopython, *all* of the data for each pubmed article starts at the root node with the key `PubmedArticle`. Within the `PubmedArticle` level, the author information is contained under `MedlineCitation` key. So now we have descended down two branches into the land of `MedlineKey`, and from here we can check if the author data was properly stored in the database retrieval (just in case something went wrong...), and if it was we can then iterate over each author and store theur first and last name in  a list a We also make sure they have both a first and a last name as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract authors from a given article\n",
    "def extract_authors(medline):\n",
    "    # define a new list to store the author names in \n",
    "    authors = [] \n",
    "    if 'AuthorList' in medline['Article']: # check if there is even an author list in the current article\n",
    "        # if there is an author list, iterate through it so we can store the names\n",
    "        for author in medline['Article']['AuthorList']: \n",
    "            # use .get to retrieve the value associated with the author- remember this from Amino acids?\n",
    "            last_name = author.get('LastName', '') \n",
    "            fore_name = author.get('ForeName', '') \n",
    "            # check if only last or first name\n",
    "            if last_name or fore_name: \n",
    "                authors.append(last_name + ', ' + fore_name) \n",
    "    # In case no authors found\n",
    "    else: \n",
    "        authors.append('Author(s) not found')\n",
    "    # Return list of the authors\n",
    "    return ', '.join(authors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Process is basically the same for abstract. We will check to see if the article has an abstract, and if it does we will join all of the strings and return it as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract abstract from a given article\n",
    "def extract_abstract(medline):\n",
    "    if 'Abstract' in medline['Article']: \n",
    "        # the abstract is a list of strings. We want to join them into one string separating them by spaces\n",
    "        return ' '.join(medline['Article']['Abstract']['AbstractText']) \n",
    "    else:\n",
    "        return 'No abstract available' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same thing for Keyword extraction. The `KeywordList` is under the `Article` section of the Dictionary Element as opposed to `Medline` however, so we pass that as an argument here. We used loop comprehension to store all the keywords in a list. It is good practice to convert this code to a normal for loop format to make sure you are really understanding list comprehension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract keywords from a given article\n",
    "def extract_keywords(article): \n",
    "    if 'KeywordList' in article and len(article['KeywordList']) > 0:\n",
    "        # if there are keywords, store them in a list and join the list with a comma (this is just a for loop, but its called loop comprehension)\n",
    "        return ', '.join([keyword for keyword in article['KeywordList'][0]]) \n",
    "    else:\n",
    "        return 'No keywords available' # return this if no keywords are found"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright this is the function we are going to call to kick off the whole chain. First we iterate through every article in our results using the keyword `PubmedArticle` as this is the root node and gives us access to everything under it we need. After that we will store all of the info under `Medline` to send to the helper functinos we defined earlier, we will extract the pubmed ID to write out later using the `PMID` key, as well as the title and publication date. All of this stuff is iterative, and we had to spend some time reading the documentation/ googling when making this lesson in order to figure it all out, so if it is not immediately clear, do not worry! Iterative googling and reading is **very** much part of the process.\n",
    "\n",
    "### We do want you to note a couple things here. The first being that we call the other functinos we defined earlier. This is kind of a meta idea that you can call functions within functions, but you totally can and should as it makes your code easier to read and understand! Functions are really just little computational tools that you can build to make your life easier!\n",
    "\n",
    "### We will store all of the data we gathered in a nice dictionary that we will return so we can easily access the data and write it later!  We also convert the datafrane (which is in a list) to somthing called a `pandas datafrane` and return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results- this is the data from the fetch_details function above!\n",
    "def extract_info(results):\n",
    "    parsed_articles = []\n",
    "    for article in results['PubmedArticle']: \n",
    "        medline = article['MedlineCitation'] \n",
    "         # extract the pubmed ID using the key PMID       \n",
    "        pmid = medline['PMID']\n",
    "        # Extract the article title (the title is nested under the article key so you have to do two layers of keys)\n",
    "        title = medline['Article']['ArticleTitle'] \n",
    "        # Get the publication date and store it in a variable\n",
    "        pub_date = medline['Article']['Journal']['JournalIssue']['PubDate'] \n",
    "        # get the authors from our helper function\n",
    "        authors = extract_authors(medline) \n",
    "        # get the abstract from our helper function\n",
    "        abstract = extract_abstract(medline) \n",
    "        # get the keywords from our helper function\n",
    "        keywords = extract_keywords(article)\n",
    "\n",
    "        parsed_articles.append({\n",
    "            'pmid': pmid,\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'pub_date': pub_date,\n",
    "            'abstract': abstract,\n",
    "            'keywords': keywords\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(parsed_articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to set up the experiment and call all the functions we just made! This is where we will get our results and reap the benefits of our hard work. This block of code is pretty self explanatory if you followed along the earlier explanations of the functions. All we are doing is deciding what keywords, number of results we want, and data raange, and then we send it down our function chain! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'Bio.Entrez.Parser.DictionaryElement'>\n"
     ]
    }
   ],
   "source": [
    "# Set up the paramters for your literature crawl\n",
    "keywords = 'artificial intelligence in medical school education'\n",
    "number = 20 \n",
    "date_begin = '2010/01/01' \n",
    "date_end = '2023/05/01'  \n",
    "\n",
    " # We call the search function and send in our parameters in order to receive the pubmed IDs from our search\n",
    "studies = search(keywords, date_begin, date_end, number)\n",
    "# We want to extract the IdList (it is in a dictionary, so we use the key associated with it)\n",
    "studiesIdList = studies['IdList'] \n",
    " # we send the Ids to our fetch_details function to get more information about the identified articles\n",
    "metadata = fetch_details(studiesIdList)\n",
    " # we extract the information that we received from the fetch_details\n",
    "data = extract_info(metadata)\n",
    "\n",
    "data.to_csv(keywords + '.csv', index=False) # we plot our data as a CSV\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And with that, if you made it through all of this then give yourself a big pat on the back. Some of this code is pretty complex, and it is great that you ar spending the time going through it. We hope you are starting to how in computer science, the hard stuff is made up of the easy stuff, just combined in various different ways. That is why it is so important to cement your foundation! Once you feel like you are really starting to understand the syntax (or at least developing an intuition and know where to look for help) you are more than ready to move on to data science. Remember- this is an iterative process! Stay with it and with time you will see results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bert-extractive-summarizer\n",
    "# from summarizer import Summarizer\n",
    "\n",
    "# def summarize_abstract(abstract):\n",
    "#     model = Summarizer()\n",
    "#     return model(abstract, num_sentences=3)\n",
    "\n",
    "# summary = summarize_abstract(abstract)\n",
    "\n",
    "# parsed_articles.append({\n",
    "#     'pmid': pmid,\n",
    "#     'title': title,\n",
    "#     'authors': ', '.join(authors),\n",
    "#     'pub_date': pub_date,\n",
    "#     'abstract': abstract,\n",
    "#     'summary' : summary,\n",
    "#     'keywords': ', '.join(keywords)\n",
    "# })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
